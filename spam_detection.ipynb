{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "#  SPAM DETECTION SYSTEM\n",
    "#  Using Statistical (TF-IDF + Naive Bayes)\n",
    "#  and Embedding-Based (Pretrained Word2Vec + Neural Network) Models\n",
    "# =========================\n",
    "\n",
    "# =========================================\n",
    "# STEP 1: SETUP AND PREPARATION\n",
    "# =========================================\n",
    "\n",
    "# ----- 1.1: Import Libraries -----\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import tensorflow as tf\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, \n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ----- 1.2: NLTK Downloads (if not already done) -----\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# ----- 1.3: Load the SMS Spam Collection Dataset -----\n",
    "# Make sure 'spam.csv' is in the working directory or update the path accordingly.\n",
    "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "\n",
    "# Check columns to ensure they exist\n",
    "expected_cols = {'v1', 'v2'}\n",
    "if not expected_cols.issubset(df.columns):\n",
    "    raise ValueError(\n",
    "        \"Expected columns 'v1' and 'v2' not found in the CSV. \"\n",
    "        \"Please verify the file structure or rename accordingly.\"\n",
    "    )\n",
    "\n",
    "# Rename columns for clarity and keep relevant ones\n",
    "df.rename(columns={'v1': 'label', 'v2': 'text'}, inplace=True)\n",
    "df = df[['label', 'text']]\n",
    "\n",
    "# Convert labels (ham/spam) to binary (0, 1)\n",
    "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "# Quick checks\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"Class distribution:\\n\", df['label'].value_counts())\n",
    "\n",
    "# =========================================\n",
    "# STEP 2: DATA PREPROCESSING\n",
    "# =========================================\n",
    "\n",
    "# ----- 2.1: Preprocessing Tools -----\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Cleans the input text by:\n",
    "      1. Lowercasing and stripping whitespace\n",
    "      2. Removing non-alphabetical characters\n",
    "      3. Tokenizing\n",
    "      4. Removing stop words\n",
    "      5. Lemmatizing words\n",
    "    Returns:\n",
    "      A list of cleaned tokens.\n",
    "    \"\"\"\n",
    "    text = text.lower().strip()\n",
    "    # If you want to keep digits or certain punctuations, adjust the regex below\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply cleaning\n",
    "df['clean_tokens'] = df['text'].apply(clean_text)\n",
    "\n",
    "# ----- 2.2: Train-Test Split -----\n",
    "X = df['clean_tokens']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\nTraining set size:\", X_train.shape[0])\n",
    "print(\"Test set size:\", X_test.shape[0])\n",
    "\n",
    "# =========================================\n",
    "# STEP 3: BASELINE MODEL (TF-IDF + NAIVE BAYES)\n",
    "# =========================================\n",
    "\n",
    "# ----- 3.1: TF-IDF Vectorization -----\n",
    "X_train_text = X_train.apply(lambda tokens: ' '.join(tokens))\n",
    "X_test_text = X_test.apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "# Adjust TF-IDF params as desired (e.g., max_features, ngram_range, etc.)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=3000, ngram_range=(1,2))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_text)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_text)\n",
    "\n",
    "# ----- 3.2: Train and Evaluate Naive Bayes -----\n",
    "nb_model = MultinomialNB(alpha=1.0)  # alpha=1.0 is the default smoothing\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred_nb = nb_model.predict(X_test_tfidf)\n",
    "\n",
    "acc_nb = accuracy_score(y_test, y_pred_nb)\n",
    "prec_nb = precision_score(y_test, y_pred_nb)\n",
    "rec_nb = recall_score(y_test, y_pred_nb)\n",
    "f1_nb = f1_score(y_test, y_pred_nb)\n",
    "\n",
    "print(\"\\n=== Baseline Model (Naive Bayes + TF-IDF) ===\")\n",
    "print(f\"Accuracy : {acc_nb:.4f}\")\n",
    "print(f\"Precision: {prec_nb:.4f}\")\n",
    "print(f\"Recall   : {rec_nb:.4f}\")\n",
    "print(f\"F1 Score : {f1_nb:.4f}\")\n",
    "\n",
    "cm_nb = confusion_matrix(y_test, y_pred_nb)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm_nb, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Naive Bayes (TF-IDF) Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# =========================================\n",
    "# STEP 4: EMBEDDING-BASED MODEL (Pretrained Word2Vec + Neural Network)\n",
    "# =========================================\n",
    "\n",
    "# ----- 4.1: Load Pretrained Word2Vec -----\n",
    "# This can be very large (~1.5GB). Ensure enough RAM/disk space.\n",
    "print(\"\\nLoading pretrained 'word2vec-google-news-300' model...\")\n",
    "pretrained_w2v = api.load('word2vec-google-news-300')\n",
    "print(\"Pretrained model loaded successfully.\")\n",
    "\n",
    "# For Gensim 4+, pretrained_w2v is a KeyedVectors instance\n",
    "# We'll fetch the dimensionality from 'vector_size'\n",
    "try:\n",
    "    vector_dim = pretrained_w2v.vector_size\n",
    "except AttributeError:\n",
    "    # Fallback for certain versions\n",
    "    vector_dim = pretrained_w2v.wv.vector_size\n",
    "\n",
    "def embed_text(tokens: list, model: gensim.models.KeyedVectors) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Given a list of tokens and a pretrained KeyedVectors model,\n",
    "    return the average embedding vector for the text.\n",
    "    If no valid tokens exist, return a zero vector.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for word in tokens:\n",
    "        # For Gensim 4.0+, we check 'key_to_index'\n",
    "        if word in model.key_to_index: \n",
    "            embeddings.append(model[word])\n",
    "    if len(embeddings) == 0:\n",
    "        return np.zeros(vector_dim)\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "X_train_embed = np.array([embed_text(tokens, pretrained_w2v) for tokens in X_train])\n",
    "X_test_embed = np.array([embed_text(tokens, pretrained_w2v) for tokens in X_test])\n",
    "\n",
    "print(\"\\nTrain Embeddings shape:\", X_train_embed.shape)\n",
    "print(\"Test Embeddings shape :\", X_test_embed.shape)\n",
    "\n",
    "# ----- 4.2: Build, Compile, and Train Neural Network -----\n",
    "model_nn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(vector_dim,)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),  # Higher dropout for limited data\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_nn.summary()\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model_nn.fit(\n",
    "    X_train_embed, \n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# ----- 4.3: Evaluate Neural Network -----\n",
    "y_pred_proba_nn = model_nn.predict(X_test_embed).ravel()\n",
    "y_pred_nn = (y_pred_proba_nn > 0.5).astype(int)\n",
    "\n",
    "acc_nn = accuracy_score(y_test, y_pred_nn)\n",
    "prec_nn = precision_score(y_test, y_pred_nn)\n",
    "rec_nn = recall_score(y_test, y_pred_nn)\n",
    "f1_nn = f1_score(y_test, y_pred_nn)\n",
    "\n",
    "print(\"\\n=== Embedding-Based Model (Pretrained Word2Vec + NN) ===\")\n",
    "print(f\"Accuracy : {acc_nn:.4f}\")\n",
    "print(f\"Precision: {prec_nn:.4f}\")\n",
    "print(f\"Recall   : {rec_nn:.4f}\")\n",
    "print(f\"F1 Score : {f1_nn:.4f}\")\n",
    "\n",
    "cm_nn = confusion_matrix(y_test, y_pred_nn)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm_nn, annot=True, fmt='d', cmap='Greens')\n",
    "plt.title(\"Neural Network (Pretrained Word2Vec) Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# =========================================\n",
    "# STEP 5: RESULTS AND ANALYSIS\n",
    "# =========================================\n",
    "\n",
    "labels = ['Naive Bayes (TF-IDF)', 'NN (Pretrained W2V)']\n",
    "accuracies = [acc_nb, acc_nn]\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(labels, accuracies, color=['blue', 'green'])\n",
    "plt.title(\"Model Comparison - Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0.8, 1.0)\n",
    "for i, v in enumerate(accuracies):\n",
    "    plt.text(i, v + 0.01, f\"{v:.2f}\", ha='center', fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "# Plot training history for the neural network\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title(\"NN Loss Curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.title(\"NN Accuracy Curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Final Results Summary ===\")\n",
    "print(\"\\nNaive Bayes (TF-IDF):\")\n",
    "print(f\"  Accuracy : {acc_nb:.4f}\")\n",
    "print(f\"  Precision: {prec_nb:.4f}\")\n",
    "print(f\"  Recall   : {rec_nb:.4f}\")\n",
    "print(f\"  F1 Score : {f1_nb:.4f}\")\n",
    "\n",
    "print(\"\\nNeural Network (Pretrained Word2Vec):\")\n",
    "print(f\"  Accuracy : {acc_nn:.4f}\")\n",
    "print(f\"  Precision: {prec_nn:.4f}\")\n",
    "print(f\"  Recall   : {rec_nn:.4f}\")\n",
    "print(f\"  F1 Score : {f1_nn:.4f}\")\n",
    "\n",
    "# =========================================\n",
    "# OPTIONAL: Threshold Tuning Example\n",
    "# (COMMENTED OUT BY DEFAULT)\n",
    "# =========================================\n",
    "# from sklearn.metrics import precision_recall_curve\n",
    "# import numpy as np\n",
    "#\n",
    "# thresholds = np.linspace(0,1,50)\n",
    "# prec_vals, rec_vals = [], []\n",
    "# for t in thresholds:\n",
    "#     y_thresh = (y_pred_proba_nn > t).astype(int)\n",
    "#     p = precision_score(y_test, y_thresh)\n",
    "#     r = recall_score(y_test, y_thresh)\n",
    "#     prec_vals.append(p)\n",
    "#     rec_vals.append(r)\n",
    "#\n",
    "# plt.figure(figsize=(6,4))\n",
    "# plt.plot(thresholds, prec_vals, label='Precision')\n",
    "# plt.plot(thresholds, rec_vals, label='Recall')\n",
    "# plt.title(\"NN Precision/Recall vs. Threshold\")\n",
    "# plt.xlabel(\"Threshold\")\n",
    "# plt.ylabel(\"Score\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "#\n",
    "# # You can pick an optimal threshold based on the trade-off \n",
    "# # or compute the F1 at different thresholds to choose the best.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
